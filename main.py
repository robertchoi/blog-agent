import streamlit as st
import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from typing import List, TypedDict

from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.graph import StateGraph, END

# --- 1. í™˜ê²½ ì„¤ì • ---
# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
load_dotenv()

# LangSmith ì¶”ì  ì„¤ì • (ì„ íƒ ì‚¬í•­) - ë¹„í™œì„±í™”
# os.environ["LANGCHAIN_TRACING_V2"] = "true"
# os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGCHAIN_API_KEY")
# os.environ["LANGCHAIN_PROJECT"] = "Multi-Agent Blog Generator"

# --- 2. ë„êµ¬ ì •ì˜ ---
# Tavilyë¥¼ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰ ë„êµ¬
tavily_tool = TavilySearchResults(max_results=5)

# URL ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘ ë„êµ¬
def scrape_web_content(url: str) -> str:
    """ì§€ì •ëœ URLì˜ ì›¹ ì½˜í…ì¸ ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        response.raise_for_status()  # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ë³¸ë¬¸ ì½˜í…ì¸  ìœ„ì£¼ë¡œ ì¶”ì¶œ (article, main íƒœê·¸ ë“±)
        main_content = soup.find('main') or soup.find('article') or soup.body
        if main_content:
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (nav, footer, script, style ë“±)
            for tag in main_content(['nav', 'footer', 'script', 'style', 'aside', 'form']):
                tag.decompose()
            text = main_content.get_text(separator='\n', strip=True)
            return text
        return "ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except requests.RequestException as e:
        return f"URL ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
    except Exception as e:
        return f"ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# --- 3. ì—ì´ì „íŠ¸ ìƒíƒœ ì •ì˜ ---
class AgentState(TypedDict):
    url: str
    scraped_content: str
    seo_analysis: str
    seo_tags: List[str]
    draft_post: str
    final_title: str
    final_subheadings: List[str]
    final_post: str
    image_prompt: str
    image_url: str
    messages: List[BaseMessage]

# --- 4. ì—ì´ì „íŠ¸ ë° ë…¸ë“œ ì •ì˜ ---
# LLM ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o", temperature=0.7)

# 4.1. ë¦¬ì„œì²˜ ì—ì´ì „íŠ¸ (URL ìŠ¤í¬ë˜í•‘)
def researcher_node(state: AgentState):
    """
    ì…ë ¥ëœ URLì˜ ì½˜í…ì¸ ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
    """
    st.write("â–¶ï¸ ë¦¬ì„œì²˜ ì—ì´ì „íŠ¸: URL ì½˜í…ì¸  ë¶„ì„ ì‹œì‘...")
    url = state['url']
    scraped_content = scrape_web_content(url)
    
    if "ì˜¤ë¥˜ ë°œìƒ" in scraped_content or "ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in scraped_content:
        st.error(f"ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {scraped_content}")
        return {
            "scraped_content": f"ë¶„ì„ ì‹¤íŒ¨: {scraped_content}",
            "messages": [HumanMessage(content=f"URL ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {url}")]
        }
        
    st.success("âœ… ë¦¬ì„œì²˜ ì—ì´ì „íŠ¸: ì½˜í…ì¸  ë¶„ì„ ì™„ë£Œ!")
    return {
        "scraped_content": scraped_content,
        "messages": [HumanMessage(content=f"URL '{url}'ì˜ ì½˜í…ì¸  ë¶„ì„ ì™„ë£Œ.")]
    }

# 4.2. SEO ì „ë¬¸ê°€ ì—ì´ì „íŠ¸
def seo_specialist_node(state: AgentState):
    """
    ìŠ¤í¬ë©ëœ ì½˜í…ì¸ ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë„¤ì´ë²„ SEO ì „ëµì„ ë¶„ì„í•˜ê³  íƒœê·¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    st.write("â–¶ï¸ SEO ì „ë¬¸ê°€ ì—ì´ì „íŠ¸: ë„¤ì´ë²„ SEO ì „ëµ ë¶„ì„ ì¤‘...")
    scraped_content = state['scraped_content']
    
    # ë„¤ì´ë²„ SEO íŠ¸ë Œë“œ ê²€ìƒ‰
    search_query = "2025ë…„ ë„¤ì´ë²„ ë¸”ë¡œê·¸ SEO ìµœì í™” ì „ëµ"
    seo_trends = tavily_tool.invoke({"query": search_query})

    prompt = ChatPromptTemplate.from_messages([
        ("system", 
         """ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ ë„¤ì´ë²„ ë¸”ë¡œê·¸ SEO ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
         ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ì£¼ì–´ì§„ ì›ë³¸ ì½˜í…ì¸ ì™€ ìµœì‹  SEO íŠ¸ë Œë“œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë„¤ì´ë²„ ê²€ìƒ‰ì— ìµœì í™”ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
         
         ì§€ì¹¨:
         1. ì›ë³¸ ì½˜í…ì¸ ì˜ í•µì‹¬ ì£¼ì œì™€ ì£¼ìš” í‚¤ì›Œë“œë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
         2. ìµœì‹  ë„¤ì´ë²„ SEO íŠ¸ë Œë“œë¥¼ ì°¸ê³ í•˜ì—¬, ì–´ë–¤ í‚¤ì›Œë“œì™€ ì£¼ì œë¥¼ ê°•ì¡°í•´ì•¼ í• ì§€ ê²°ì •í•©ë‹ˆë‹¤.
         3. ì‚¬ìš©ìë“¤ì´ ê²€ìƒ‰í•  ë§Œí•œ ë§¤ë ¥ì ì´ê³  êµ¬ì²´ì ì¸ ë¡±í…Œì¼ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ì œëª©ê³¼ ì†Œì œëª© ì•„ì´ë””ì–´ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
         4. ë„¤ì´ë²„ ë¸”ë¡œê·¸ì— ì‚¬ìš©ë  SEOì— ê°€ì¥ íš¨ê³¼ì ì¸ íƒœê·¸ 10ê°œë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì œê³µí•©ë‹ˆë‹¤.
         5. ëª¨ë“  ê²°ê³¼ë¬¼ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
         
         ê²°ê³¼ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
         
         [ë¶„ì„ ë° ì „ëµ]
         - (ì—¬ê¸°ì— ì½˜í…ì¸ ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ SEO ì „ëµê³¼ í‚¤ì›Œë“œ ë¶„ì„ ë‚´ìš©ì„ ì„œìˆ )
         
         [ì¶”ì²œ íƒœê·¸]
         #íƒœê·¸1, #íƒœê·¸2, #íƒœê·¸3, #íƒœê·¸4, #íƒœê·¸5, #íƒœê·¸6, #íƒœê·¸7, #íƒœê·¸8, #íƒœê·¸9, #íƒœê·¸10
         """),
        ("human", 
         "**ìµœì‹  ë„¤ì´ë²„ SEO íŠ¸ë Œë“œ:**\n{seo_trends}\n\n"
         "**ë¶„ì„í•  ì›ë³¸ ì½˜í…ì¸ :**\n{scraped_content}"),
    ])
    
    chain = prompt | llm
    # Pass variables to the prompt template
    response = chain.invoke({
        "seo_trends": seo_trends,
        "scraped_content": scraped_content[:4000]
    })
    
    # ê²°ê³¼ íŒŒì‹±
    analysis_text = response.content
    tags_part = analysis_text.split("[ì¶”ì²œ íƒœê·¸]")[1].strip()
    tags = [tag.strip() for tag in tags_part.split(", ")]
    
    st.success("âœ… SEO ì „ë¬¸ê°€ ì—ì´ì „íŠ¸: ì „ëµ ë¶„ì„ ë° íƒœê·¸ ìƒì„± ì™„ë£Œ!")
    return {
        "seo_analysis": analysis_text,
        "seo_tags": tags
    }

# 4.3. ì‘ì„±ê°€ ì—ì´ì „íŠ¸
def writer_node(state: AgentState):
    """
    SEO ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì´ˆì•ˆì„ ì‘ì„±í•©ë‹ˆë‹¤.
    """
    st.write("â–¶ï¸ ì‘ì„±ê°€ ì—ì´ì „íŠ¸: ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì´ˆì•ˆ ì‘ì„± ì¤‘...")
    scraped_content = state['scraped_content']
    seo_analysis = state['seo_analysis']
    
    # 1ê°œì˜ ì¶”ì²œ ì œëª©ì„ ìƒì„±
    title_prompt = ChatPromptTemplate.from_messages([
        ("system", 
         """ë‹¹ì‹ ì€ ë„¤ì´ë²„ ë¸”ë¡œê·¸ SEO ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì½˜í…ì¸ ì™€ SEO ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ í´ë¦­ì„ ìœ ë„í•˜ëŠ” ë§¤ë ¥ì ì¸ ì œëª©ì„ ë§Œë“œëŠ” ê²ƒì´ ì„ë¬´ì…ë‹ˆë‹¤.
         
         ìš”êµ¬ì‚¬í•­:
         - SEO í‚¤ì›Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
         - í˜¸ê¸°ì‹¬ì„ ìê·¹í•˜ëŠ” í‘œí˜„ ì‚¬ìš©
         - ë„¤ì´ë²„ ê²€ìƒ‰ì— ìµœì í™”ëœ ê¸¸ì´ (30-40ì)
         - ë§¤ë ¥ì ì´ê³  í´ë¦­ë¥ ì´ ë†’ì€ í•˜ë‚˜ì˜ ì œëª© ì œì•ˆ
         
         ê²°ê³¼ëŠ” ì œëª©ë§Œ ì¶œë ¥í•˜ì„¸ìš” (ì¶”ê°€ ì„¤ëª… ì—†ì´).
         """),
        ("human", 
         "**SEO ì „ë¬¸ê°€ ë¶„ì„ ë° ì „ëµ:**\n{seo_analysis}\n\n"
         "**ì°¸ê³ í•  ì›ë³¸ ì½˜í…ì¸ :**\n{scraped_content}"),
    ])
    
    title_chain = title_prompt | llm
    main_title = title_chain.invoke({
        "seo_analysis": seo_analysis,
        "scraped_content": scraped_content[:4000]
    }).content.strip()
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", 
         """ë‹¹ì‹ ì€ ì‚¬ëŒë“¤ì˜ ì‹œì„ ì„ ì‚¬ë¡œì¡ëŠ” ê¸€ì„ ì“°ëŠ” ì „ë¬¸ ë¸”ë¡œê·¸ ì‘ê°€ì…ë‹ˆë‹¤. ë„¤ì´ë²„ ë¸”ë¡œê·¸ í”Œë«í¼ì˜ íŠ¹ì„±ì„ ì˜ ì´í•´í•˜ê³  ìˆìŠµë‹ˆë‹¤.
         ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ì œëª©ê³¼ SEO ì „ë¬¸ê°€ì˜ ë¶„ì„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë…ìë“¤ì´ ì‰½ê²Œ ì½ê³  ê³µê°í•  ìˆ˜ ìˆëŠ” ë§¤ë ¥ì ì¸ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
         
         ì‘ì„± ê°€ì´ë“œë¼ì¸:
         1. **ì œëª©:** ì£¼ì–´ì§„ ì œëª©ì„ `#`ìœ¼ë¡œ ì‹œì‘í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.
         2. **ì†Œê°œ:** ë…ìì˜ í¥ë¯¸ë¥¼ ìœ ë°œí•˜ê³  ê¸€ì„ ê³„ì† ì½ê³  ì‹¶ê²Œ ë§Œë“œëŠ” ë„ì…ë¶€ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
         3. **ë³¸ë¬¸:** SEO ì „ë¬¸ê°€ê°€ ì œì•ˆí•œ ì†Œì œëª© ì•„ì´ë””ì–´ë¥¼ í™œìš©í•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ ì†Œì œëª©(`##`)ìœ¼ë¡œ ë¬¸ë‹¨ì„ ë‚˜ëˆ„ì„¸ìš”. ê° ë¬¸ë‹¨ì€ ì›ë³¸ ì½˜í…ì¸ ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•˜ë˜, ë” ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë¬¸ì²´ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤. ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì—¬ì£¼ì„¸ìš”.
         4. **ê²°ë¡ :** ê¸€ì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ê³ , ë…ìì—ê²Œ í–‰ë™ì„ ìœ ë„í•˜ê±°ë‚˜ ê¸ì •ì ì¸ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•˜ë©° ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.
         5. **ìŠ¤íƒ€ì¼:** ì „ì²´ì ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ëŒ€í™”í•˜ëŠ” ë“¯í•œ í†¤ì•¤ë§¤ë„ˆë¥¼ ìœ ì§€í•˜ê³ , ê° í† í”½ì€ 500ì ì´ìƒ 1000ì ì´í•˜ë¡œ ì‘ì„±í•´ì£¼ìƒˆìš”
         """),
        ("human", 
         "**ì‚¬ìš©í•  ì œëª©:**\n{title}\n\n"
         "**SEO ì „ë¬¸ê°€ ë¶„ì„ ë° ì „ëµ:**\n{seo_analysis}\n\n"
         "**ì°¸ê³ í•  ì›ë³¸ ì½˜í…ì¸ :**\n{scraped_content}"),
    ])
    
    chain = prompt | llm
    draft_post = chain.invoke({
        "title": main_title,
        "seo_analysis": seo_analysis,
        "scraped_content": scraped_content[:4000]
    }).content
    
    # ì†Œì œëª© ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    lines = draft_post.split('\n')
    subheadings = []
    for line in lines:
        if line.startswith('## '):
            subheadings.append(line.replace('## ', '').strip())
            
    st.success("âœ… ì‘ì„±ê°€ ì—ì´ì „íŠ¸: í¬ìŠ¤íŠ¸ ì´ˆì•ˆ ì‘ì„± ì™„ë£Œ!")
    return {
        "draft_post": draft_post,
        "final_title": main_title,
        "final_subheadings": subheadings
    }

# 4.4. ì•„íŠ¸ ë””ë ‰í„° ì—ì´ì „íŠ¸
def art_director_node(state: AgentState):
    """
    ë¸”ë¡œê·¸ ì œëª©ê³¼ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ DALL-Eë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    st.write("â–¶ï¸ ì•„íŠ¸ ë””ë ‰í„° ì—ì´ì „íŠ¸: ëŒ€í‘œ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
    title = state['final_title']
    draft_post = state['draft_post']

    # DALL-E í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt_generator_llm = ChatOpenAI(model="gpt-4o", temperature=0.7)
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", "ë‹¹ì‹ ì€ ì°½ì˜ì ì¸ ì•„íŠ¸ ë””ë ‰í„°ì…ë‹ˆë‹¤. ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì˜ ì œëª©ê³¼ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, DALL-E 3ê°€ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” ê°€ì¥ íš¨ê³¼ì ì´ê³  ìƒì„¸í•œ ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤."),
        ("human", f"ë¸”ë¡œê·¸ ì œëª©: {title}\n\në¸”ë¡œê·¸ ë‚´ìš© ìš”ì•½:\n{draft_post[:500]}\n\nìœ„ ë‚´ìš©ì„ ëŒ€í‘œí•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.")
    ])
    
    chain = prompt_template | prompt_generator_llm
    # No variables needed since we use f-strings in the template
    image_prompt = chain.invoke({}).content

    # ì´ë¯¸ì§€ ìƒì„± í´ë¼ì´ì–¸íŠ¸
    from openai import OpenAI
    client = OpenAI()

    try:
        response = client.images.generate(
            model="dall-e-3",
            prompt=image_prompt,
            size="1024x1024",
            quality="standard",
            n=1,
        )
        image_url = response.data[0].url
        st.success("âœ… ì•„íŠ¸ ë””ë ‰í„° ì—ì´ì „íŠ¸: ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ!")
        return {"image_prompt": image_prompt, "image_url": image_url}
    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return {"image_prompt": image_prompt, "image_url": "ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨"}

# --- 5. ê·¸ë˜í”„ ë¹Œë“œ ---
def build_graph():
    workflow = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("researcher", researcher_node)
    workflow.add_node("seo_specialist", seo_specialist_node)
    workflow.add_node("writer", writer_node)
    workflow.add_node("art_director", art_director_node)
    
    # ì—£ì§€ ì—°ê²° (ìˆœì°¨ì  ì‹¤í–‰)
    workflow.set_entry_point("researcher")
    workflow.add_edge("researcher", "seo_specialist")
    workflow.add_edge("seo_specialist", "writer")
    workflow.add_edge("writer", "art_director")
    workflow.add_edge("art_director", END)
    
    return workflow.compile()

# --- 6. ìŠ¤íŠ¸ë¦¼ë¦¿ UI ---
def main():
    st.set_page_config(page_title="ğŸ¤– ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ìŠ¤íŒ… ìë™ ìƒì„±ê¸°", layout="wide")
    st.title("ğŸ¤– ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ìŠ¤íŒ… ìë™ ìƒì„±ê¸°")
    st.markdown("""
    **ì°¸ê³ í•  ê¸°ì‚¬ë‚˜ ë¸”ë¡œê·¸ ê¸€ì˜ URLì„ ì…ë ¥**í•˜ë©´, AI ì—ì´ì „íŠ¸ë“¤ì´ í˜‘ë ¥í•˜ì—¬ **ë„¤ì´ë²„ SEOì— ìµœì í™”ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸**ë¥¼ ìë™ìœ¼ë¡œ ë§Œë“¤ì–´ ë“œë¦½ë‹ˆë‹¤. 
    ì´ ë„êµ¬ëŠ” **ì½˜í…ì¸  ì œì‘ ì‹œê°„ì„ ë‹¨ì¶•**í•˜ê³  **ê²€ìƒ‰ ë…¸ì¶œì„ ì¦ëŒ€**ì‹œí‚¤ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
    """)

    url = st.text_input("ë¶„ì„í•  ê¸°ì‚¬ ë˜ëŠ” ë¸”ë¡œê·¸ URLì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="https://...")

    if st.button("ğŸš€ ë¸”ë¡œê·¸ ê¸€ ìƒì„± ì‹œì‘!"):
        if not url:
            st.warning("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return

        with st.spinner("AI ë©€í‹°ì—ì´ì „íŠ¸ê°€ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
            app = build_graph()
            
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state = {"url": url, "messages": []}
            
            # ê·¸ë˜í”„ ì‹¤í–‰
            final_state = app.invoke(initial_state)

        st.divider()
        st.header("âœ¨ ìµœì¢… ê²°ê³¼ë¬¼ âœ¨")

        # 1. ìƒì„±ëœ ì´ë¯¸ì§€ í‘œì‹œ
        if final_state.get("image_url") and final_state["image_url"] != "ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨":
            st.image(final_state["image_url"], caption=f"DALL-E Prompt: {final_state.get('image_prompt', 'N/A')}")
        else:
            st.warning("ëŒ€í‘œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # 2. ì¶”ì²œ ì œëª© ë° íƒœê·¸ í‘œì‹œ
        st.subheader("ğŸ“ ì¶”ì²œ ì œëª©")
        st.code(final_state.get('final_title', 'ì œëª© ìƒì„± ì‹¤íŒ¨'), language=None)

        st.subheader("ğŸ”– ì¶”ì²œ íƒœê·¸ (ë³µì‚¬í•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”)")
        tags_str = ", ".join([f"#{tag}" for tag in final_state.get('seo_tags', [])])
        st.code(tags_str, language=None)
        
        # 3. ì™„ì„±ëœ ë¸”ë¡œê·¸ ê¸€ í‘œì‹œ
        st.subheader("âœï¸ ì™„ì„±ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ (ë§ˆí¬ë‹¤ìš´)")
        st.markdown(final_state.get('draft_post', 'í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨'))

        # 4. ìƒì„¸ ë¶„ì„ ë‚´ìš© (ë””ë²„ê¹…/ì°¸ê³ ìš©)
        with st.expander("ğŸ¤– ì—ì´ì „íŠ¸ ì‘ì—… ìƒì„¸ ë‚´ìš© ë³´ê¸°"):
            st.write("**SEO ì „ë¬¸ê°€ ë¶„ì„:**")
            st.text(final_state.get('seo_analysis', 'ë¶„ì„ ë‚´ìš© ì—†ìŒ'))

if __name__ == "__main__":
    main()